{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 1, Asymmetric loss regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Let's load the data and plot the training data out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x106e323d0>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106e24c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "from utils import *  # this loads definitions of functions etc. in utils.py provided with the assignment\n",
    "import pylab\n",
    "\n",
    "# Load data\n",
    "X_train, y_train = loadData('train')\n",
    "X_val, y_val = loadData('val')\n",
    "\n",
    "pylab.plot(X_train, y_train, '.')\n",
    "pylab.plot(X_val, y_val, 'r.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will solve the linear regression (under the \"normal\" symmetric squared loss) using the closed form solution. Let's define a couple of functions we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def symmLoss(X, w ,y):\n",
    "    \"\"\"\n",
    "    Get the symmetric squared loss given data X, weight w and ground truth y\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        N x d+1 data matrix (row per example)\n",
    "    w : 1D array\n",
    "        d+1 length vector\n",
    "    y : 1D array\n",
    "        Observed function values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : a scalar\n",
    "        The loss calculated by the symmetric loss formula\n",
    "    \"\"\"\n",
    "    return YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lsqClosedForm(X, y):\n",
    "    \"\"\"\n",
    "    Use closed form solution for least squares minimization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        N x d+1 data matrix (row per example)\n",
    "    y : 1D array\n",
    "        Observed function values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : 1D array\n",
    "        d+1 length vector\n",
    "    \"\"\"\n",
    "    return np.dot(np.linalg.pinv(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the closed form solution: generate a toy data set from a random linear function with no noise. We should be able to perfectly recover w in this case (up to numerical precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones([20,1]),np.random.random((20,1))))\n",
    "w = np.random.random((2))\n",
    "y = np.dot(X,w)\n",
    "print('true weight:  '+repr(w))\n",
    "w_ = lsqClosedForm(X, y)\n",
    "print('function output: '+repr(w_))\n",
    "if (np.allclose(w,w_)):\n",
    "    print('Close enough')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to estimate the variance of the noise and the log likelihood of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logLikelihood(X, w, y):\n",
    "    \"\"\"\n",
    "    Get the estimated variance, and the log likelihood of the data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        N x d+1 design matrix (row per example)\n",
    "    w : 1D array\n",
    "        d+1 length vector\n",
    "    y : 1D array\n",
    "        Observed function values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    simga2 : a scalar\n",
    "        The estimated variance (sigma squared)\n",
    "    loglike : a scalar\n",
    "        The log-likelihood under the Gaussian noise model N(0,sigma2)\n",
    "    \"\"\"\n",
    "    N = X.shape[0]   # number of rows in X\n",
    "    # now estimate the variance of the Gaussian noise (sigma2 stands for \\sigma^2)\n",
    "    sigma2 = YOUR CODE HERE\n",
    "    # normalized log-likelihood (mean of per-data point log-likelihood of the model given by w,sigma2)\n",
    "    loglike = - 1 / 2.0 * np.log(2*np.pi*sigma2) - np.mean((np.dot(X, w) - y)**2) / (sigma2 * 2)\n",
    "    return sigma2, loglike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit linear, quadratic and cubic models to the training data, and plot the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_loss = np.Inf\n",
    "pylab.plot(X_val, y_val, 'k.')\n",
    "\n",
    "# Try degree 1 to 3\n",
    "for deg in [1,2,3]:\n",
    "    # Expand data first; you can check how this function works in utils.py\n",
    "    X, C = degexpand(X_train, deg)\n",
    "    \n",
    "    # Get the result by applying normal equation\n",
    "    w = lsqClosedForm(X, y_train)\n",
    "    \n",
    "    # compute loss on training\n",
    "    loss = symmLoss(X, w, y_train)\n",
    "    \n",
    "    # compute loss on val; note -- use the same scaling matrix C as for training\n",
    "    val_loss = symmLoss(degexpand(X_val, deg, C)[0], w, y_val)\n",
    "    print('degree %d:' %(deg))\n",
    "    print('train loss %.6f' %(loss))\n",
    "    print('val loss %.6f' %(val_loss))\n",
    "    print('sigma^2: %.6f \\nlog-likelihood %.6f\\n' %logLikelihood(X, w, y_train))\n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        # record in best_param the model weights, degree, and the scaling matrix of the best model so far\n",
    "        best_param = (w, deg, C)\n",
    "    \n",
    "    # Plot the function\n",
    "    color = {1:'b', 2:'g', 3:'r'}[deg]\n",
    "    pylab.plot(np.linspace(min(X_train)-.1,max(X_train)+.1), np.dot(degexpand(np.linspace(min(X_train)-.1,max(X_train)+.1).reshape((50, 1)), deg, C)[0], w), color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Announce result on test data\n",
    "X_test, y_test = loadData('test')\n",
    "print(\"Best degree:\"+repr(best_param[1]))\n",
    "YOUR CODE HERE TO PRINT RELEVANT EVALUATION METRIC FOR THE CHOSEN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to repeat the experiment above but under the asymmetric loss function. Since there is no closed form solution, we will need to rely on gradient descent. First we need to implement the loss function and the gradient function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def asymmLoss(X, w, y,alpha):\n",
    "    \"\"\"\n",
    "    Get the asymmetric loss given data X, weight w and ground truth y\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        N x d+1 design matrix (row per example)\n",
    "    w : 1D array\n",
    "        d+1 length vector\n",
    "    y : 1D array\n",
    "        Observed function values\n",
    "    alpha : scalar\n",
    "        weight put on positive error, i.e., yhat > y\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : a scalar\n",
    "        The loss calculated by equation in problem set 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # it may be more convenient to define the loss as a per-data point weighted loss,\n",
    "    # with weights determined by the sign of the error and collected into a diagonal matrix\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    yhat = np.dot(X, w)\n",
    "    weights=np.zeros([N,N])\n",
    "    for i in range(0,N):\n",
    "        if yhat[i] > y[i]:\n",
    "            weights[i][i] = alpha\n",
    "        else:\n",
    "            weights[i][i] = 1\n",
    "    loss = YOUR CODE HERE\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def asymmGrad(X, w, y,alpha):\n",
    "    \"\"\"\n",
    "    Get the gradient of w\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        N x d+1 design matrix (row per example)\n",
    "    w : 1D array\n",
    "        d+1 length vector\n",
    "    y : 1D array\n",
    "        Observed function values\n",
    "    alpha : scalar\n",
    "        weight put on positive error, i.e., yhat > y\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad : 1D array\n",
    "        d+1 length vector\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    yhat = np.dot(X,w)\n",
    "    # use the weights here as well, defined by alpha\n",
    "    weights=np.zeros([N,N])\n",
    "    for i in range(0,N):\n",
    "        if yhat[i] > y[i]:\n",
    "            weights[i][i] = alpha\n",
    "        else:\n",
    "            weights[i][i] = 1\n",
    "    grad = YOUR CODE HERE\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the loss and gradient function. You can manually verify that for the given values of X (2 data points), y and w, with alpha=10, you should get these numbers for the objective (asymmetric loss) value and for the gradient. Then, run the code to make sure your implementation of the gradient is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss, grad = asymmLoss(np.array([[1,2], [1, -2]]), np.array([1,1]), np.array([4, -2]),10), \\\n",
    "            asymmGrad(np.array([[1,2], [1, -2]]), np.array([1,1]), np.array([4, -2]),10)\n",
    "print(\"expected output\")\n",
    "print(\"5.5\")\n",
    "print(\"[ 9 -22]\")\n",
    "print(\"function output:\")\n",
    "print(loss)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have had the functions to calculate loss and gradient, we can implement the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradDescent(X, y,alpha,tol=1e-4,maxIt=10000):\n",
    "    \"\"\"\n",
    "    Use gradient descent to min(loss(X, w, y))\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        N x d+1 design matrix (row per example)\n",
    "    y : 1D array\n",
    "        Observed function values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : 1D array\n",
    "        d+1 length vector\n",
    "        \n",
    "    it: number of iterations until convergence\n",
    "    \"\"\"\n",
    "    # Random initialize the weight\n",
    "    w = np.random.randn(X.shape[1])\n",
    "    lr = 0.01 # learning rate (make it constant 0.01; feel free to experiment with the value)\n",
    "    it = 0 # iteration count\n",
    "    lastloss = np.Inf # loss computed at previous check point\n",
    "    checkit = 500 # interval to check convergence\n",
    "    while True:\n",
    "        loss, grad = asymmLoss(X, w, y,alpha), asymmGrad(X, w, y,alpha)\n",
    "        \n",
    "        w = YOUR CODE HERE for updating w\n",
    "        \n",
    "        \n",
    "        it += 1 # advance iteration count\n",
    "        \n",
    "        if it % checkit == 0: # check point -- evaluate progress and decide whether to stop\n",
    "            converged = it >= maxIt or loss > lastloss-tol\n",
    "            lastloss = loss\n",
    "            print('iter %d:  loss %.4f' %(it,loss))\n",
    "            if converged:\n",
    "                break\n",
    "        \n",
    "    return w, it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test gradient descent using any alpha and data generated by (random) noiseless linear model; we should recover the true w fairly accurately (although possibly with less accuracy than the closed form solution for alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones([20,1]),np.random.random((20,1))))\n",
    "w = np.random.random((2))\n",
    "y = np.dot(X,w)\n",
    "print('true weight:'+repr(w))\n",
    "w_, it_ = gradDescent(X, y,10,1e-6,10000)\n",
    "print('%d iterations' %it_)\n",
    "print('function output:'+repr(w_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit different models, and evaluate their performance on train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_loss = np.Inf\n",
    "# Try degree 1 to 3\n",
    "pylab.plot(X_val, y_val, 'k.')\n",
    "for deg in [1,2,3]:\n",
    "    # Expand data first; you can check how this function works in utils.py\n",
    "    X, C = degexpand(X_train, deg)\n",
    "    y = y_train\n",
    "        \n",
    "    # Do gradient descent\n",
    "    w, _ = gradDescent(X, y,10)\n",
    "    loss = asymmLoss(X, w, y,10)\n",
    "    val_loss = asymmLoss(degexpand(X_val, deg, C)[0], w, y_val,10)\n",
    "    \n",
    "    print('degree %d:' %(deg))\n",
    "    print('train loss %.6f' %(loss))\n",
    "    print('val loss %.6f' %(val_loss))\n",
    "    print('sigma^2: %.6f \\nlog-likelihood %.6f\\n' %logLikelihood(X, w, y_train))\n",
    "    \n",
    "\n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        best_param = (w, deg, C)\n",
    "    \n",
    "    # Plot the function\n",
    "    color = {1:'b', 2:'g', 3:'r'}[deg]\n",
    "    pylab.plot(np.linspace(min(X_train)-.1,max(X_train)+.1), np.dot(degexpand(np.linspace(min(X_train)-.1,max(X_train)+.1).reshape((50, 1)), deg, C)[0], w), color + '--')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Announce result on test data\n",
    "X_test, y_test = loadData('test')\n",
    "print(\"Best degree:\"+repr(best_param[1]))\n",
    "YOUR CODE HERE TO PRINT RELEVANT EVALUATION METRIC FOR THE CHOSEN MODEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
