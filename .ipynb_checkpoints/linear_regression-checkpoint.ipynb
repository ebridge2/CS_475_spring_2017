{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "*by Eric Bridgeford*\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of linear regression is simple: for any real valued, multivariate input, provide the \"most-likely\" real valued output. Stated another way:\n",
    "\n",
    "Given: $\\left\\{(x, y)\\right\\}_{i=1}^N, x_i \\in \\mathbb{R}^d, y \\in \\mathbb{R}$\n",
    "Can we find $\\textrm{argmin}_w L(w | x, y)$, or the weights for our model that will minimize our loss for each training example?\n",
    "\n",
    "### Model\n",
    "\n",
    "Essentially, with the OLS solution, we assume that our data can be described by a multivariate equation, \n",
    "\\begin{align*}\n",
    "    Y_{N, 1} &= X_{N, d}W_{d, 1} + \\epsilon_{N, 1} \\\\\n",
    "    Y_{N, 1} &= \\hat{Y}_{N, 1} + \\epsilon_{N, 1}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\hat{Y} = XW$. Let's look a little more closely at what this means for us:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\begin{bmatrix} y_1\\\\\n",
    "    \\vdots \\\\\n",
    "    y_N\n",
    "    \\end{bmatrix}\n",
    "    &= \n",
    "    \\begin{bmatrix}\n",
    "        1 & x_1^{(1)} &\\;...\\; &x_1^{(d)} \\\\\n",
    "        \\vdots & \\vdots & \\;...\\; &\\vdots \\\\\n",
    "        1 & x_N^{(1)} &\\;...\\; &x_N^{(d)}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        w_0 \\\\\n",
    "        \\vdots \\\\\n",
    "        w_d\n",
    "    \\end{bmatrix} +\n",
    "    \\begin{bmatrix}\n",
    "    \\epsilon_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\epsilon_N\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "Previously, we mention this $\\textrm{argmin}_w L(w | x, y)$ that we call a loss function. While here we only need one parameter, $w$, this can be generalized to virtually any learning algorithm. What this says is, given data $x$, and expected outputs $y$, how can we find the parameters $\\theta = \\left\\{\\theta_1, ...\\theta_m\\right\\}$ to maximize the fit of our data?\n",
    "\n",
    "### Ordinary Least Squares (OLS) Loss Function\n",
    "\n",
    "By far, the most common solution to the loss-function question for linear regression is the least-squares function. You may have heard this referred to by a number of names: sum of squared errors, sum of squared residuals, OLS, etc; these names all point to the same function. \n",
    "\n",
    "We can see that just by moving some terms around from our model, we will see that $\\epsilon = Y - \\hat{Y}$, or that the error (aka, the residuals) is just the difference between our observations and the predictions (indicated by the $\\hat{Y}$) that we can make given $W$. We can interpret this graphically (in a 2D example) as:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b0/Linear_least_squares_example2.svg\">\n",
    "\n",
    "Where we can see our predicted $\\hat{Y}$ in blue, our goal $Y$ in red, and our residual/errors $\\epsilon$ in green.\n",
    "\n",
    "To \"sum\" the squared errors/residuals, we simply consider:\n",
    "\n",
    "\\begin{align*}\n",
    "    SSE = \\frac{1}{2}\\sum_{i=1}^N (\\epsilon_i)^2 &= \\frac{1}{2}\\sum_{i=1}^N (y_i - x_i w)^2 \\\\\n",
    "    \\frac{1}{2}\\sum_{i=1}^N(y_i - \\hat{y}_i)^2&= \\frac{1}{2}\\left(\\sum_{i=1}^N\\left(y_i - \\sum_{j=1}^{d+1}x_i^{(j)}w_j\\right)\\right)^2 \\\\\n",
    "    &= ||y - \\hat{y}||_2^2\n",
    "\\end{align*}\n",
    "\n",
    "We will ignore the $\\frac{1}{2}$ for now since it is just a constant, and therefore will not impact any calculus we do. The notation $||\\cdot||_2^2$ is just the traditional squared $L-2$ norm you are all used to given input $\\cdot$; the Euclidian length of a vector in euclidian space is an example of a $L-2$ norm.\n",
    "\n",
    "Where $x_i  = \\begin{bmatrix} 1,& x_i^{(1)},& \\;...\\;&x_i^{(d)}\\end{bmatrix} \\in \\mathbb{R}^{1, d+1}$ is just the rows from our matrix $X$ above, and $w = \\begin{bmatrix} w_0, & w_1, &\\;...\\;w_d \\end{bmatrix}^T \\in \\mathbb{R}^{d+1, 1}$ is just our column vector of weights. Using matrices, we can very easily rewrite this as the quadratic:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\epsilon^T\\epsilon &= (Y - \\hat{Y})^T(Y - \\hat{Y}) \\\\\n",
    "    2SSE &= (Y - XW)^T(Y - XW) \\\\\n",
    "    &= (Y^T - W^TX^T)(Y - XW) \\\\\n",
    "    &= Y^TY - Y^TXW - W^TX^TY + W^TX^TXW\n",
    "\\end{align*}\n",
    "\n",
    "We know that the value we are looking for is going to be a scalar (as we are considering $SSE \\in \\mathbb{R}^1$). Then, it is easy to see that since the transpose of any scalar is also a scalar, that we can simply let $-2Y^TX = (-2Y^TX)^T = -2X^TY$.\n",
    "\n",
    "\\begin{align*}\n",
    "    SSE &=  Y^TY- 2Y^TXW + X^TW^TXW \n",
    "\\end{align*}\n",
    "\n",
    "Finally, we can restate our optimization problem:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{W} = \\textrm{argmin}_W SSE(W | X, Y)\n",
    "\\end{align*}\n",
    "\n",
    "or, given our training examples $X$ and our expected predictions $Y$, can we find weights $W$ that minimize our squared-error loss? We differentiate with respect to $W$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\delta SSE}{\\delta W} &= \\frac{\\delta (Y^TY - 2Y^TXW + X^TW^TXW)}{\\delta W}\n",
    "\\end{align*}\n",
    "\n",
    "As far as taking matrix derivatives goes. Linear derivatives are exactly like univariate calculus. We notice that $-2Y^TXW$ is linear in $W$, so the derivative with respect to $W$ is simply $-2Y^TX$. The quadratic form $X^TW^TXW$ is a little different, but still pretty simple. Just drop the $W^T$ term, and multiply by $2$ like you are used to in univariate calculus.\n",
    "\n",
    "\\begin{align*}\n",
    "    0 &= -2Y^TX + 2X^TXW \\\\\n",
    "    -2X^TXW &= -2X^TY \\\\\n",
    "    (X^TX)^{-1}X^TXW &= (X^TX)^{-1}X^TY \\\\\n",
    "    \\hat{W} &= (X^TX)^{-1}X^TY\n",
    "\\end{align*}\n",
    "\n",
    "where our $W$ is just our \"best fit\" $\\hat{W}$ based on the loss function we described.\n",
    "\n",
    "## Training\n",
    "\n",
    "To \"train\" the a linear regression classifier with the least-squares loss function, just use the closed-form solution above to compute the best weights $\\hat{W}$ that minimize the squared error given your inputs $X$ and outputs $Y$.\n",
    "\n",
    "## Prediction\n",
    "\n",
    "To make a prediction, once we have our $\\hat{W}$, we can simply dot our new input $X_{new}$ with our weight vector $\\hat{W}$ that just have just trained:\n",
    "\n",
    "\\begin{align*}\n",
    "    y_{prediction} = x_{new}W\n",
    "\\end{align*}\n",
    "\n",
    "Where our prediction $y_{prediction} \\in \\mathbb{R}^1$, $x_{new} \\in \\mathbb{R}^{1, d}$.\n",
    "\n",
    "## Regularization\n",
    "\n",
    "To regularize our model, we will look at the simple ridge regression model (see tutorial [regularization](./regularization.ipynb)).It is easy to derive a new solution using the same derivation from above:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{W} &= \\textrm{argmin}_W \\left[RSS(W | X, Y) + \\frac{\\lambda}{2}||W||_2^2\\right] \\\\\n",
    "    &\\rightarrow \\frac{1}{2}(Y - XW)^T(Y - XW)^T + \\frac{\\lambda}{2}W^TW \\\\\n",
    "    RLS &= Y^TY - 2Y^TXW + X^TW^TXW + \\lambda W^TW\n",
    "\\end{align*}\n",
    "\n",
    "Where again we ignore the $\\frac{1}{2}$ since it is only there in the first place for notational convenience when taking the derivatives. Do not get confused by RSS (Residual Sum of Squares, or the loss function for the Ordinary Least Squares solution) and RLS (regularized least squares, or the regularized version of the OLS solution). Our solution becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\delta RLS}{\\delta W} &= \\frac{Y^TY - 2Y^TXW + X^TW^TXW + \\lambda W^TW}{\\delta W} \\\\\n",
    "    0 &= -2X^TY + 2X^TYW + 2\\lambda I W \\\\\n",
    "    \\hat{W} &= (X^TX + \\lambda I)^{-1}X^TY\n",
    "\\end{align*}\n",
    "\n",
    "Where $I$ is just the identity matrix.\n",
    "\n",
    "## Writing an Algorithm\n",
    "\n",
    "There is no standard procedure to writing an algorithm; literally anyone can do it, creating a problem: how can we trust that a given algorithm performs to our standards?\n",
    "\n",
    "There is no easy solution to this question, however, an investigator I work with, Dr. Josh Vogelstein in the BME/AMS department, has taken a stab at it. Here is a link to [methods_paper](https://github.com/neurodata/checklists/blob/master/methods_paper.md) put together by Jovo, which essentially introduces the most basic framework for producing an algorithm and being able to demonstrate to others that it unambiguously achieves some goal that you set out for it. Essentially, this methods introduction provides all of the information an investigator might need to trust your algorithm. These aspects are: setting (what does your algorithm seek to satisfy?), pseudocode (what does the actual algorithm do?), synthetic tests (does your algorithm perform when you would expect it to?), and real data (how does it do on an actual dataset?). These aspects should, intuitively, be purely foundational for any code segment, but it is especially critical for a Machine Learning algorithm to satisfy these requirements, otherwise, your algorithm is worthless!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
